import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

from dataloader.pt_data_loader.specialdatasets import StandardDataset
import dataloader.pt_data_loader.mytransforms as mytransforms

NUM_SAMPLES = 3

def print_dataset(dataloader):
    """ Loads the first NUM_SAMPLES elements in the dataloder and prints their keys and shapes.

    :param dataloader: Dataloader object
    """
    i = 0
    for element in dataloader:
        for key in element.keys():
            if key == 'filename':
                print("Key: {}, First element: {}".format(key, element[key][list(element[key].keys())[0]]))
            else:
                print("Key: {}, Element Size: {}".format(key, element[key].shape))
        i += 1
        if i >= NUM_SAMPLES:
            break


def check_dataset(dataset_name, split=None, trainvaltest_split='train', keys_to_load=None, folders_to_load=None):
    """ Loads a dataset and prints name and shape of the first NUM_SAMPLES entries. Performs no transforms other than
    the necessary ones.

    :param dataset_name: Name of the dataset
    :param split: Name of the dataset split, if one exists
    :param trainvaltest_split: 'train', 'validation' or 'test'
    :param keys_to_load: keys that are supposed to be loaded, e.g. 'color', 'depth', 'segmentation', ...
    """
    dataset = dataset_name
    data_transforms = [mytransforms.CreateScaledImage(),
                       mytransforms.CreateColoraug(),
                       mytransforms.ToTensor(),
                       ]
    if keys_to_load is not None:
        if any('depth' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertDepth())
        if any('segmentation' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertSegmentation())
        if any('flow' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertFlow())
    print('\n Loading {} dataset'.format(dataset))
    my_dataset = StandardDataset(dataset,
                                   split=split,
                                   trainvaltest_split=trainvaltest_split,
                                   keys_to_load=keys_to_load,
                                   data_transforms=data_transforms,
                                   folders_to_load=folders_to_load
                                   )
    my_loader = DataLoader(my_dataset, batch_size=1,
                             shuffle=False, num_workers=1, pin_memory=True, drop_last=True)
    print('Number of elements: {}'.format(len(my_dataset)))
    print_dataset(my_loader)


def check_scaled_dataset(dataset_name, scaled_dataset_name, trainvaltest_split, keys_to_load, scaled_size, split=None):
    """ Checks whether the images in a dataset generated by the dataset_scaler are identical to the images that are
    generated by loading the original dataset and scaling them afterwards

    :param dataset_name: Name of the unscaled dataset
    :param scaled_dataset_name: Name of the scaled dataset
    :param trainvaltest_split: 'train', 'validation' or 'test'
    :param keys_to_load: keys that are supposed to be loaded, e.g. 'color', 'depth', 'segmentation', ...
    :param scaled_size: Size of the scaled image (h, w)
    :param split: Name of the dataset split, if one exists
    """
    dataset = dataset_name
    data_transforms = [mytransforms.CreateScaledImage(),
                       mytransforms.Resize(output_size=scaled_size),
                       mytransforms.CreateColoraug(),
                       mytransforms.ToTensor(),
                       ]
    if keys_to_load is not None:
        if any('depth' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertDepth())
        if any('segmentation' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertSegmentation())
    print('\n Loading {} dataset'.format(dataset))
    my_dataset = StandardDataset(dataset,
                                   split=split,
                                   trainvaltest_split=trainvaltest_split,
                                   keys_to_load=keys_to_load,
                                   data_transforms=data_transforms,
                                   output_filenames=True
                                   )
    my_loader = DataLoader(my_dataset, batch_size=1,
                             shuffle=False, num_workers=0, pin_memory=True, drop_last=True)
    print_dataset(my_loader)

    dataset_s = scaled_dataset_name
    data_transforms = [mytransforms.CreateScaledImage(),
                       mytransforms.CreateColoraug(),
                       mytransforms.ToTensor(),
                       ]
    if keys_to_load is not None:
        if any('depth' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertDepth())
        if any('segmentation' in key for key in keys_to_load):
            data_transforms.insert(0, mytransforms.ConvertSegmentation())
    print('\n Loading {} dataset'.format(dataset_s))
    my_dataset_s = StandardDataset(dataset_s,
                                     split=split,
                                     trainvaltest_split=trainvaltest_split,
                                     keys_to_load=keys_to_load,
                                     data_transforms=data_transforms,
                                     output_filenames=True
                                     )
    my_loader_s = DataLoader(my_dataset_s, batch_size=1,
                               shuffle=False, num_workers=0, pin_memory=True, drop_last=True)
    print_dataset(my_loader_s)
    print("Testing dataset_scaler")
    samples = []
    samples_s = []
    iter_my_loader = iter(my_loader)
    iter_my_loader_s = iter(my_loader_s)
    for _ in range(2):
        samples.append(next(iter_my_loader).copy())
        samples_s.append(next(iter_my_loader_s).copy())
    for key in keys_to_load:
        print("Check if {} entries are equal:".format(key))
        print("  Should be False: {}".format(
            torch.equal(samples[1][(key, 0, 0)], samples_s[0][(key, 0, 0)])))
        print("  Should be True: {}".format(
            torch.equal(samples[0][(key, 0, 0)], samples_s[0][(key, 0, 0)])))
        print("  Should be True: {}".format(
            torch.equal(samples[1][(key, 0, 0)], samples_s[1][(key, 0, 0)])))


if __name__ == '__main__':
    check_dataset('bdd100k', keys_to_load=['color', 'segmentation'])
    check_dataset('gta5')
    check_dataset('cityscapes', keys_to_load=['color', 'depth', 'segmentation'],
                  folders_to_load=['bremen', 'darmstadt', 'erfurt'])
    check_dataset('cityscapes', keys_to_load=['color', 'depth', 'segmentation'],
                  folders_to_load=['ulm', 'strasbourg', 'hanover'])
    check_dataset('cityscapes', keys_to_load=['color', 'depth', 'segmentation'],
                  folders_to_load=['aachen', 'zurich', 'tubingen'])
    check_dataset('kitti', split='eigen_split', keys_to_load=['color', 'depth'])
    check_dataset('synthia', keys_to_load=['color', 'depth', 'segmentation'])
    check_dataset('kitti_2015', keys_to_load=['color', 'depth', 'flow'])

    check_scaled_dataset('gta5', 'gta5_mross_scaled', trainvaltest_split='train',
                       keys_to_load=['color', 'segmentation'], scaled_size=(526, 957))
    check_scaled_dataset('make3d', 'make3d_mross_scaled_2', trainvaltest_split='train',
                         keys_to_load=['color', 'depth'], scaled_size=(1136, 852))
    check_scaled_dataset('virtual_kitti', 'virtual_kitti_mross_scaled', trainvaltest_split='train',
                         keys_to_load=['color', 'depth', 'segmentation'], scaled_size=(187, 621), split="full_split")
    check_scaled_dataset('synthia', 'synthia_mross_scaled', trainvaltest_split='train',
                        keys_to_load=['color', 'depth', 'segmentation'], scaled_size=(380, 640))
    check_scaled_dataset('bdd100k', 'bdd100k_mross_scaled', trainvaltest_split='train',
                        keys_to_load=['color', 'segmentation'], scaled_size=(360, 640))